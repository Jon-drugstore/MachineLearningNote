# <center>Data Engineering</center>



<br></br>

![](./Images/data_eng1.jpg)

<br></br>



## 数据清洗
----
### 数据采样
对分类问题，选取正例，负例。对回归问题，需采集数据。对于采样得到的样本，根据需要，设定样本权重。当模型不能使用全部数据训练时，需对数据采样，设定一定采样率。采样方法包括随机采样、固定比例采样等方法。

<br>


### 样本过滤
1. 结合业务情况进行数据过滤，例如去除crawler抓取，spam，作弊等数据。
2. 异常点检测，对样本分析，常用异常检测算法包括：

    * 偏差检测，例如聚类，最近邻等。
    * 基于统计的异常点检测算法，如极差，四分位数间距，均差，标准差等。适合于挖掘单变量数值型数据。全距，又称极差，表示统计资料变异量数(measures of variation) ，最大值与最小值差距。四分位距用来构建箱形图，及对概率分布的简要图表概述。
    * 基于距离异常点检测算法。主要通过距离方法检测，将数据集中与多数点距离大于阈值的点视为异常点。主要使用的距离度量有绝对距离（曼哈顿距离）、欧氏距离和马氏距离等。
    * 基于密度的异常点检测算法，考察当前点周围密度，可发现局部异常点，例如LOF算法。

<br></br>



## 特征处理
----
### 归一化
不同特征有不同取值范围，有些算法中，如线性模型或距离相关的模型，特征取值范围对最终结果产生影响。如二元特征的取值范围为$$[0，1]$$，而距离特征取值可能是$$[0，正无穷)$$。实际使用中会对距离截断，例如$$[0，3000000]$$。但这两个特征由于取值范围不一致导致模型更偏向取值范围较大的特征。

为平衡取值范围不一致的特征，需对特征归一化处理，归一化到$$[0，1]$$。常用的归一化方法包括：
1. 函数归一化。通过映射函数将特征取值映射到$$[0，1]$$。例如最大最小值归一化方法，是线性映射。还有通过非线性函数映射，如log函数。
2. 分维度归一化。可使用最大最小归一化方法，但最大最小值选取的是所属类别的，即局部最大最小值。
3. 排序归一化。不管原来特征取值什么样，特征按大小排序，根据特征所对应的序给予新值。

<br>


### 离散化
连续值取值空间可能无穷，为便于表示和处理，需对连续值离散化处理。常用的离散化方法包括等值划分和等量划分。

等值划分将特征按照值域进行均分，每一段内取值等同处理。如特征取值范围为$$[0，10]$$，将其分为10段，$$[0，1)$$，$$[1，2)$$，...，$$[9，10)$$。

等量划分是根据样本总数进行均分。如距离特征，取值范围$$[0，3000000]$$，需分成10段。如果按等比例划分，大部分样本都在第1段。使用等量划分会避免这种问题，最终可能的切分是$$[0，100)$$，$$[100，300)$$，$$[300，500)$$，..，$$[10000，3000000]$$，前面的区间划分比较密，后面的比较稀疏。

<br>


### 缺省值处理
缺省值如何赋予，有很多方法。如单独表示，众数，平均值等。

以下是关于如何处理特征向量缺失值：
- 缺失值较多

    直接将特征舍弃。

- 缺失值较少

    缺失值较少，其余特征缺失值都在10%以内，可以采取很多方式处理：

    1. 把`NaN`作为特征，假设用0表示，`data_train.fillna(0)`。
    2. 用均值填充。均值填充可能需取条件均值，如训练集中患癌症和不患癌症数据中，该值差距大，应填充label相同的数据均值。`data_train.fillna(data_train.mean())`
    3. 用上下数据填充。
    4. 插值法。即估计中间点的值，`data_train.interpolate()`。